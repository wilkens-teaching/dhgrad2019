{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP and entity extraction\n",
    "\n",
    "## Summary\n",
    "\n",
    "Use the SpaCy NLP library to identify named entities in a short text, then write code to parse the  output.\n",
    "\n",
    "## Details\n",
    "\n",
    "The text in question is one we've already seen (in the sutructured data exercise). To review: it's a brief pamphlet, *Growler's Income Tax* (1864), by the prolific mid-nineteenth-century writer T.S. Arthur. It's a defense of the then-new income tax, instituted in 1861 to fund the Union's war effort. As you'll see, the text is pretty straightforward, but it's kind of nifty (or infuriating, I guess) how similar are the arguments it presents concerning taxation to those you might hear today. Go ahead, download the [plain-text copy](https://github.com/wilkens-teaching/dhgrad2019/blob/master/exercises/ps4/growler.txt) and read it now, if you haven't already. It's short. (Note that you could also work with the [XML version](https://github.com/wilkens-teaching/dhgrad2019/blob/master/exercises/ps3/growler.xml) from the previous exercise and convert that to plain text as shown in the [solution to that exercise](https://github.com/wilkens-teaching/dhgrad2019/blob/master/exercises/ps3/Parsing%20structured%20data%20solution.ipynb).)\n",
    "\n",
    "Anyway, tax policy isn't really the point. Your task is to identify algorithmically the named entities in the text and to extract them for further processing. To do this, you'll use the SpaCy NLP library. SpaCy isn't included with the default Anaconda distribution, but you can install it via the Anaconda Navigator GUI or from the command line (or from a console within Jupyter lab) by typing:\n",
    "\n",
    "```\n",
    "conda install spacy\n",
    "```\n",
    "\n",
    "and then installing at least the basic trained model:\n",
    "\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "SpaCy is a deep-learning-based NLP package. But the underlying details aren't super important for our purposes; we're interested in how to use it and in how it performs on our text document. You can see the [full usage instructions](https://spacy.io/usage) at the SpaCy site; there's also some starter code below.\n",
    "\n",
    "Once you've processed the document, your task is to write code that reads the processed data and builds a list of unique entities in the output, each entity's type, and a count of how many times each entity occurs.\n",
    "\n",
    "Your program should print a summary of the entity data. Your output (with made-up data) should look roughly like this:\n",
    "\n",
    "```\n",
    "Entity\t\tType\t\tCount\n",
    "------\t\t----\t\t-----\n",
    "Boston\t\tLocation\t  2\n",
    "John Smith\tPerson\t\t1\n",
    "```\n",
    "\n",
    "## Alternative processing\n",
    "If you're up for a modest challenge, you might try using NLTK's (slower, lower-performing) named entity chunker. But that's strictly optional.\n",
    "\n",
    "## Submit\n",
    "\n",
    "Submit your code and output as a single Jupyter notebook via Sakai.\n",
    "\n",
    "## Consider\n",
    "\n",
    "A few things to think about before class:\n",
    "\n",
    "* How well or poorly do the entities extracted from the text square with your sense of what the text is about, whom it involves, and where it occurs (or with what areas itâ€™s concerned)?\n",
    "* How accurate is the NER process?\n",
    "* How might you try to improve NER accuracy?\n",
    "\n",
    "## Starter code\n",
    "\n",
    "Here's a bit of code to get you going. Note that most of these examples involve placeholder filenames and will require changes to run on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of a text file from disk\n",
    "with open('growler.txt', 'r') as f:\n",
    "    txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('George Washington', 'PERSON'), ('first', 'ORDINAL'), ('the United States', 'GPE'), ('Virginia', 'GPE'), ('1732', 'DATE'), ('December of 1799', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "# NLP with SpaCy\n",
    "import spacy\n",
    "import en_core_web_sm # Remember to download this model before importing!\n",
    "\n",
    "txt = \"\"\"George Washington was the first president of the United States. He was born in Virginia in 1732 and died in December of 1799.\"\"\"\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(txt)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag\tentity\tcount\n",
      "a\tthing 1\t2\n",
      "b\tthing 3\t2\n",
      "b\tthing 2\t1\n"
     ]
    }
   ],
   "source": [
    "# Count things\n",
    "from collections import defaultdict\n",
    "\n",
    "data = [\n",
    "    ('thing 1', 'a'), \n",
    "    ('thing 2', 'b'), \n",
    "    ('thing 3', 'b'), \n",
    "    ('thing 1', 'a'), \n",
    "    ('thing 3', 'b')\n",
    "]\n",
    "\n",
    "counts = defaultdict(dict)\n",
    "\n",
    "for item in data:\n",
    "    try:\n",
    "        counts[item[1]][item[0]] += 1\n",
    "    except:\n",
    "        counts[item[1]][item[0]] = 1\n",
    "\n",
    "# Print sorted counts\n",
    "print('tag\\tentity\\tcount')\n",
    "for tag in counts:\n",
    "    for entity in sorted(counts[tag], key=counts[tag].get, reverse=True):\n",
    "        print(f'{tag}\\t{entity}\\t{counts[tag][entity]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your code here\n",
    "\n",
    "Use the examples above to read the document from disk, process it with SpaCy, and produce a list of counted entities by type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag\t\tentity\t\tcount\n",
      "---\t\t------\t\t-----\n",
      "ORG\t\tGROWLER\t\t1\n",
      "ORG\t\tESQ\t\t1\n",
      "ORG\t\tDistrict of Pennsylvania.\t\t1\n",
      "ORG\t\tRec'd\t\t1\n",
      "ORG\t\tGovernment\t\t1\n",
      "ORG\t\tState\t\t1\n",
      "ORG\t\tConfederation\t\t1\n",
      "ORG\t\tthe Union\t\t1\n",
      "\n",
      "GPE\t\tT.S.\t\t1\n",
      "GPE\t\tPhiladelphia\t\t1\n",
      "GPE\t\tPennsylvania\t\t1\n",
      "GPE\t\tAlabama\t\t1\n",
      "GPE\t\tFlorida\t\t1\n",
      "GPE\t\tDelaware\t\t1\n",
      "GPE\t\tEngland\t\t1\n",
      "GPE\t\tFrance\t\t1\n",
      "GPE\t\tCarlisle\t\t1\n",
      "GPE\t\tGettysburg\t\t1\n",
      "\n",
      "PERSON\t\tGrowler\t\t13\n",
      "PERSON\t\tCollector\t\t2\n",
      "PERSON\t\tLee\t\t2\n",
      "PERSON\t\tStand\t\t1\n",
      "PERSON\t\tPistols\t\t1\n",
      "PERSON\t\tRICHARD GROWLER\t\t1\n",
      "PERSON\t\tJOHN M. RILEY\t\t1\n",
      "PERSON\t\tI.\t\t1\n",
      "PERSON\t\tG.\t\t1\n",
      "PERSON\t\tCollector Riley's\t\t1\n",
      "\n",
      "WORK_OF_ART\t\tWar Tax\t\t1\n",
      "\n",
      "DATE\t\tSept.,\t\t1\n",
      "DATE\t\t1863\t\t1\n",
      "DATE\t\tthe year 1862\t\t1\n",
      "DATE\t\tmany years\t\t1\n",
      "DATE\t\tannual\t\t1\n",
      "DATE\t\tYesterday\t\t1\n",
      "DATE\t\tnext year\t\t1\n",
      "\n",
      "ORDINAL\t\t4th\t\t1\n",
      "\n",
      "MONEY\t\tforty-three dollars\t\t4\n",
      "MONEY\t\ttwenty-one cents\t\t4\n",
      "MONEY\t\tfifty-eight dollars\t\t3\n",
      "MONEY\t\tFifty-eight dollars\t\t2\n",
      "MONEY\t\t43,21\t\t1\n",
      "MONEY\t\tJust forty-three dollars\t\t1\n",
      "MONEY\t\ttwo hundred dollars\t\t1\n",
      "MONEY\t\tnearly two thousand dollars\t\t1\n",
      "MONEY\t\tA million dollars\t\t1\n",
      "MONEY\t\ta hundred dollars\t\t1\n",
      "MONEY\t\tover a thousand million dollars\t\t1\n",
      "\n",
      "CARDINAL\t\thalf\t\t2\n",
      "CARDINAL\t\tmore than half\t\t2\n",
      "CARDINAL\t\tsix\t\t2\n",
      "CARDINAL\t\tone\t\t1\n",
      "CARDINAL\t\ta thousand\t\t1\n",
      "CARDINAL\t\tHalf\t\t1\n",
      "CARDINAL\t\tmore than three\t\t1\n",
      "CARDINAL\t\tforty-three\t\t1\n",
      "CARDINAL\t\tThousands\t\t1\n",
      "CARDINAL\t\ttens of thousand\t\t1\n",
      "\n",
      "QUANTITY\t\tone half or two thirds\t\t1\n",
      "QUANTITY\t\tover two hundred bushels\t\t1\n",
      "\n",
      "LOC\t\tSouth\t\t1\n",
      "LOC\t\tRappahannock\t\t1\n",
      "\n",
      "NORP\t\tSouthern\t\t1\n",
      "NORP\t\tAmerican\t\t1\n",
      "\n",
      "EVENT\t\t\"Nearer ten millions of dollars\t\t1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('growler.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "    doc = nlp(txt)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    counts = defaultdict(dict)\n",
    "    for entity, tag in entities:\n",
    "        entity = entity.replace('\\n', '')\n",
    "        if len(entity) > 0:\n",
    "            try:\n",
    "                counts[tag][entity] += 1\n",
    "            except:\n",
    "                counts[tag][entity] = 1\n",
    "    print('tag\\t\\tentity\\t\\tcount')\n",
    "    print('---\\t\\t------\\t\\t-----')\n",
    "    for tag in counts:\n",
    "        for entity in sorted(counts[tag], key=counts[tag].get, reverse=True):\n",
    "            print(f'{tag}\\t\\t{entity}\\t\\t{counts[tag][entity]}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequently-occurring entities\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tag       entity             \n",
       "PERSON    Growler                13\n",
       "MONEY     twenty one cents        4\n",
       "          forty three dollars     4\n",
       "          fifty eight dollars     3\n",
       "CARDINAL  more than half          2\n",
       "PERSON    Collector               2\n",
       "CARDINAL  six                     2\n",
       "MONEY     Fifty eight dollars     2\n",
       "PERSON    Lee                     2\n",
       "CARDINAL  half                    2\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pandas alternative\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "punct = frozenset(list(string.punctuation + '\\n'))\n",
    "\n",
    "def strip_punct(phrase):\n",
    "    new = ''\n",
    "    for char in phrase:\n",
    "        if char not in punct:\n",
    "            new += char\n",
    "        else:\n",
    "            new += ' '\n",
    "    new = new.strip()\n",
    "    if len(new) > 0:\n",
    "        return new.strip()\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "df = pd.DataFrame.from_records(\n",
    "    entities,\n",
    "    columns=['entity', 'tag']\n",
    ")\n",
    "\n",
    "df['entity'] = df['entity'].apply(strip_punct)\n",
    "\n",
    "g = df.groupby(['tag', 'entity']).size()\n",
    "\n",
    "print(\"Most frequently-occurring entities\\n\")\n",
    "with pd.option_context('display.max_rows', 999):\n",
    "    display(g.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top entities by type\n",
      "\n",
      "CARDINAL\n",
      "entity\n",
      "six               2\n",
      "more than half    2\n",
      "half              2\n",
      "dtype: int64\n",
      "\n",
      "DATE\n",
      "entity\n",
      "the year 1862    1\n",
      "next year        1\n",
      "many years       1\n",
      "dtype: int64\n",
      "\n",
      "EVENT\n",
      "entity\n",
      "Nearer ten millions of dollars    1\n",
      "dtype: int64\n",
      "\n",
      "GPE\n",
      "entity\n",
      "T S             1\n",
      "Philadelphia    1\n",
      "Pennsylvania    1\n",
      "dtype: int64\n",
      "\n",
      "LOC\n",
      "entity\n",
      "South           1\n",
      "Rappahannock    1\n",
      "dtype: int64\n",
      "\n",
      "MONEY\n",
      "entity\n",
      "twenty one cents       4\n",
      "forty three dollars    4\n",
      "fifty eight dollars    3\n",
      "dtype: int64\n",
      "\n",
      "NORP\n",
      "entity\n",
      "Southern    1\n",
      "American    1\n",
      "dtype: int64\n",
      "\n",
      "ORDINAL\n",
      "entity\n",
      "4th    1\n",
      "dtype: int64\n",
      "\n",
      "ORG\n",
      "entity\n",
      "the Union    1\n",
      "State        1\n",
      "Rec d        1\n",
      "dtype: int64\n",
      "\n",
      "PERSON\n",
      "entity\n",
      "Growler      13\n",
      "Lee           2\n",
      "Collector     2\n",
      "dtype: int64\n",
      "\n",
      "QUANTITY\n",
      "entity\n",
      "over two hundred bushels    1\n",
      "one half or two thirds      1\n",
      "dtype: int64\n",
      "\n",
      "WORK_OF_ART\n",
      "entity\n",
      "War Tax    1\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top entities by type\\n\")\n",
    "for tag, group in df.groupby(['tag']):\n",
    "    print(tag)\n",
    "    print(group.groupby(['entity']).size().sort_values(ascending=False).head(3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A-Alcott-Little_Women-1868-F.txt\n",
      "A-Cather-Antonia-1918-F.txt\n",
      "A-Chesnutt-Marrow-1901-M.txt\n",
      "A-Crane-Maggie-1893-M.txt\n",
      "A-Dreiser-Sister_Carrie-1900-M.txt\n",
      "A-Hawthorne-Scarlet_Letter-1850-M.txt\n",
      "A-Howells-Silas_Lapham-1885-M.txt\n",
      "A-James-Golden_Bowl-1904-M.txt\n",
      "A-London-Call_Wild-1903-M.txt\n",
      "A-Jewett-Pointed_Firs-1896-F.txt\n",
      "A-Melville-Moby_Dick-1851-M.txt\n",
      "A-Norris-Pit-1903-M.txt\n",
      "A-Twain-Huck_Finn-1885-M.txt\n",
      "A-Wharton-Age_Innocence-1920-F.txt\n",
      "A-Chopin-Awakening-1899-F.txt\n",
      "A-Gilman-Herland-1915-F.txt\n",
      "A-Harper-Iola_Leroy-1892-F.txt\n",
      "A-Stowe-Uncle_Tom-1852-F.txt\n",
      "A-Freeman-Pembroke-1894-F.txt\n",
      "A-Davis-Life_Iron_mills-1861-F.txt\n",
      "CPU times: user 11min 26s, sys: 1min 46s, total: 13min 12s\n",
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The American corpus\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "\n",
    "punct = frozenset(list(string.punctuation + '\\n' + 'â€œ'))\n",
    "\n",
    "def strip_punct(phrase):\n",
    "    new = ''\n",
    "    for char in phrase:\n",
    "        if char not in punct:\n",
    "            new += char\n",
    "        else:\n",
    "            new += ' '\n",
    "    new = new.strip()\n",
    "    if len(new) > 0:\n",
    "        return new\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in glob.glob(os.path.join('..', '..', 'data', 'texts')+'/A-*.txt'):\n",
    "    name = os.path.basename(file).strip('.txt')\n",
    "    print(os.path.basename(file))\n",
    "    with open(file, 'r') as f:\n",
    "        doc = nlp(f.read())\n",
    "        entities = [(ent.text, ent.label_, name) for ent in doc.ents]\n",
    "        df = pd.DataFrame.from_records(\n",
    "            entities,\n",
    "            columns=['entity', 'tag', 'doc']\n",
    "        )\n",
    "        df['entity'] = df['entity'].apply(strip_punct)\n",
    "        dfs.append(df)\n",
    "\n",
    "results = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>tag</th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>DATE</td>\n",
       "      <td>A-Alcott-Little_Women-1868-F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>DATE</td>\n",
       "      <td>A-Alcott-Little_Women-1868-F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jo</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>A-Alcott-Little_Women-1868-F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meg</td>\n",
       "      <td>ORG</td>\n",
       "      <td>A-Alcott-Little_Women-1868-F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amy</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>A-Alcott-Little_Women-1868-F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      entity     tag                           doc\n",
       "0  Christmas    DATE  A-Alcott-Little_Women-1868-F\n",
       "1  Christmas    DATE  A-Alcott-Little_Women-1868-F\n",
       "2         Jo  PERSON  A-Alcott-Little_Women-1868-F\n",
       "3        Meg     ORG  A-Alcott-Little_Women-1868-F\n",
       "4        Amy  PERSON  A-Alcott-Little_Women-1868-F"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARDINAL\n",
      "entity\n",
      "one     2963\n",
      "two     1523\n",
      "half     628\n",
      "dtype: int64\n",
      "\n",
      "DATE\n",
      "entity\n",
      "May        286\n",
      "the day    195\n",
      "March      166\n",
      "dtype: int64\n",
      "\n",
      "EVENT\n",
      "entity\n",
      "Hugo                        10\n",
      "the Battle of the Street     5\n",
      "New Year s                   4\n",
      "dtype: int64\n",
      "\n",
      "FAC\n",
      "entity\n",
      "Eaton Square    46\n",
      "Broadway        42\n",
      "Fifth Avenue    39\n",
      "dtype: int64\n",
      "\n",
      "GPE\n",
      "entity\n",
      "Charlotte    1088\n",
      "Drouet        384\n",
      "New York      273\n",
      "dtype: int64\n",
      "\n",
      "LANGUAGE\n",
      "entity\n",
      "English      149\n",
      "Hurstwood     42\n",
      "French        19\n",
      "dtype: int64\n",
      "\n",
      "LAW\n",
      "entity\n",
      "the Countess Olenska    15\n",
      "Miranda                  4\n",
      "CHAPTER 1                2\n",
      "dtype: int64\n",
      "\n",
      "LOC\n",
      "entity\n",
      "South          98\n",
      "New England    70\n",
      "Hurstwood      70\n",
      "dtype: int64\n",
      "\n",
      "MONEY\n",
      "entity\n",
      "a cent        24\n",
      "ten cents     23\n",
      "every cent    13\n",
      "dtype: int64\n",
      "\n",
      "NORP\n",
      "entity\n",
      "Hurstwood    221\n",
      "American     156\n",
      "Christian    132\n",
      "dtype: int64\n",
      "\n",
      "ORDINAL\n",
      "entity\n",
      "first     1635\n",
      "second     273\n",
      "third      111\n",
      "dtype: int64\n",
      "\n",
      "ORG\n",
      "entity\n",
      "Meg     480\n",
      "Ahab    399\n",
      "Eva     197\n",
      "dtype: int64\n",
      "\n",
      "PERCENT\n",
      "entity\n",
      "one    2\n",
      "dtype: int64\n",
      "\n",
      "PERSON\n",
      "entity\n",
      "Carrie    1326\n",
      "Jo        1277\n",
      "Tom       1094\n",
      "dtype: int64\n",
      "\n",
      "PRODUCT\n",
      "entity\n",
      "Maggie    206\n",
      "Flask      60\n",
      "Eliza      43\n",
      "dtype: int64\n",
      "\n",
      "QUANTITY\n",
      "entity\n",
      "a mile       13\n",
      "two          11\n",
      "ten miles     7\n",
      "dtype: int64\n",
      "\n",
      "TIME\n",
      "entity\n",
      "night        261\n",
      "morning      170\n",
      "the night    157\n",
      "dtype: int64\n",
      "\n",
      "WORK_OF_ART\n",
      "entity\n",
      "Bible        93\n",
      "Hurstwood    51\n",
      "Persis       28\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tag, group in results.groupby(['tag']):\n",
    "    print(tag)\n",
    "    print(group.groupby(['entity']).size().sort_values(ascending=False).head(3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity\n",
      "Charlotte    1088\n",
      "Drouet        384\n",
      "New York      273\n",
      "Antonia       266\n",
      "Masâ€™r         222\n",
      "Rebecca       199\n",
      "Chicago       159\n",
      "Iola          133\n",
      "Sylvia        124\n",
      "St  Clare     114\n",
      "Starbuck      110\n",
      "Pit           104\n",
      "London        101\n",
      "Hurstwood      92\n",
      "Paris          91\n",
      "Jadwin         89\n",
      "Jimmie         88\n",
      "Boston         82\n",
      "Thou           79\n",
      "America        78\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for tag, group in results.loc[results.tag=='GPE'].groupby(['tag']):\n",
    "    print(group.groupby(['entity']).size().sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
